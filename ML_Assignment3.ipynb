{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree"
      ],
      "metadata": {
        "id": "GNmiX-PZF3C1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "- A decision tree is a flowchart-like model that splits data into subsets using feature-based yes/no questions to reach a prediction at leaf nodes. For classification, it recursively chooses the feature and threshold that best separate classes (e.g., information gain), creating internal decision nodes and branches until stopping criteria are met (like max depth, min samples, or pure leaves). At inference, a sample traverses the tree by following the rules at each node, and the leaf’s majority class (or class probability from class counts) becomes the predicted label. Pruning and regularization (depth limits, min samples per split/leaf) help prevent overfitting, while ensembles like Random Forests and Gradient Boosted Trees improve accuracy and robustness.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "- Gini impurity and entropy both quantify how mixed the class labels are within a node. Gini impurity measures the probability of misclassification if a label is randomly assigned according to class frequencies: Gini = 1 − Σ p(k)^2, favoring splits that create nodes with high class purity and is slightly biased toward larger class separations with faster computation.\n",
        "\n",
        "- Entropy from information theory measures uncertainty: Entropy = −Σ p(k) log2 p(k), and information gain is the reduction in entropy after a split.\n",
        "\n",
        "- In decision trees, the algorithm evaluates candidate splits and selects the one that yields the largest impurity reduction (highest Gini decrease or highest information gain), producing purer child nodes; both typically select similar splits, though entropy can be more sensitive to changes near perfectly balanced mixes, while Gini is computationally simpler and often preferred in practice.\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "- Pre-pruning halts tree growth during training using constraints like max depth, min samples per split/leaf, or min impurity decrease, preventing overly specific splits from ever forming; its practical advantage is faster training/inference with lower risk of overfitting on small datasets. Post-pruning first grows a large tree and then trims back branches based on validation performance or complexity penalties (e.g., reduced error pruning, CCP/α), merging subtrees that don’t improve generalization; its practical advantage is typically better accuracy–complexity trade-offs because pruning decisions use global, data-driven evaluation rather than local stopping rules.\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "- Information Gain is the reduction in impurity (uncertainty) achieved by splitting a node, typically measured using entropy: IG(parent, split) = Entropy(parent) − weighted sum of Entropy(children). It quantifies how much a feature/threshold improves class purity, so the tree chooses the split with the highest information gain to most effectively separate classes. This leads to purer child nodes, shorter trees, and better generalization by prioritizing splits that reveal the most informative structure in the data.\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "- Decision trees are widely used in real-world applications such as medical diagnosis (classifying diseases), credit scoring, customer churn prediction, fraud detection, and decision support systems. Their main advantages include interpretability (easy-to-understand logic), ability to handle both numerical and categorical data, and minimal data preprocessing requirements. However, they can easily overfit, especially on noisy data, and are sensitive to small changes in the data, potentially leading to unstable trees. Additionally, decision trees may struggle with modeling complex, non-linear relationships compared to ensemble or deep learning methods."
      ],
      "metadata": {
        "id": "o8f-PmWcF7aF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DfdhjZ8EFx3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78e9e67-8406-44ed-b4e3-0f6ab5bfc755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333\n",
            "petal length (cm): 0.5586\n",
            "petal width (cm): 0.4060\n",
            "sepal width (cm): 0.0292\n",
            "sepal length (cm): 0.0062\n"
          ]
        }
      ],
      "source": [
        "\"\"\" 6. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances \"\"\"\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "clf = DecisionTreeClassifier(\n",
        "    criterion=\"gini\",\n",
        "    random_state=42\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "for name, imp in sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{name}: {imp:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 7. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree. \"\"\"\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "clf_depth3 = DecisionTreeClassifier(\n",
        "    criterion=\"gini\",\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ").fit(X_train, y_train)\n",
        "\n",
        "clf_full = DecisionTreeClassifier(\n",
        "    criterion=\"gini\",\n",
        "    random_state=42\n",
        ").fit(X_train, y_train)\n",
        "\n",
        "acc_depth3 = accuracy_score(y_test, clf_depth3.predict(X_test))\n",
        "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
        "\n",
        "print(f\"Accuracy (max_depth=3): {acc_depth3:.4f}\")\n",
        "print(f\"Accuracy (fully-grown): {acc_full:.4f}\")\n",
        "\n",
        "train_acc_depth3 = accuracy_score(y_train, clf_depth3.predict(X_train))\n",
        "train_acc_full = accuracy_score(y_train, clf_full.predict(X_train))\n",
        "print(f\"Train Accuracy (max_depth=3): {train_acc_depth3:.4f}\")\n",
        "print(f\"Train Accuracy (fully-grown): {train_acc_full:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2dB5uz75dKd",
        "outputId": "6b62d484-ad26-4e00-d532-df0b46e83b93"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (max_depth=3): 0.9667\n",
            "Accuracy (fully-grown): 0.9333\n",
            "Train Accuracy (max_depth=3): 0.9833\n",
            "Train Accuracy (fully-grown): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 8. Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances \"\"\"\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "reg = DecisionTreeRegressor(\n",
        "    random_state=42\n",
        ")\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "importances = reg.feature_importances_\n",
        "sorted_importances = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(\"Feature importances:\")\n",
        "for name, imp in sorted_importances:\n",
        "    print(f\"{name}: {imp:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayHGB1IO58hC",
        "outputId": "2285dfca-d565-4469-fa0b-7ced9c95a8ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.4952\n",
            "Feature importances:\n",
            "MedInc: 0.5285\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n",
            "AveRooms: 0.0530\n",
            "HouseAge: 0.0519\n",
            "Population: 0.0305\n",
            "AveBedrms: 0.0287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 9. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy \"\"\"\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "param_grid = {\n",
        "    \"max_depth\": [None, 2, 3, 4, 5, 6, 8, 10],\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 8, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=clf,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    refit=True,\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_clf = grid.best_estimator_\n",
        "y_pred = best_clf.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "print(f\"CV Best Score (mean accuracy): {grid.best_score_:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0csdajew6X2f",
        "outputId": "e39799b4-7199-47f2-cadc-c81023deeec2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "CV Best Score (mean accuracy): 0.9417\n",
            "Test Accuracy: 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "-  Handle missing values: audit missingness; impute numerics with median and add “was_missing” flags; impute categoricals with most frequent or a “Missing” category; keep imputation inside a train-only pipeline to avoid leakage.\n",
        "- Encode categoricals: one-hot low-cardinality features (handle_unknown=ignore); for high-cardinality features consider target/ordinal encoding with leakage-safe (CV) schemes.\n",
        "-  Train model: build a Pipeline with ColumnTransformer (numeric imputer + categorical encoder) feeding DecisionTreeClassifier (use class_weight=\"balanced\" if imbalanced); use stratified train/validation/test split.\n",
        "-  Tune hyperparameters: cross-validate over max_depth, min_samples_split, min_samples_leaf, max_features, and min_impurity_decrease using randomized/grid search optimized for business-relevant metric (e.g., recall/F2).\n",
        "-  Evaluate: report confusion matrix, precision/recall/F1, ROC-AUC and PR-AUC; calibrate probabilities if thresholding risk; check subgroup fairness and stability. Business value: earlier risk flagging for targeted diagnostics, better resource prioritization, interpretable decision support for clinicians, and population-level monitoring that can reduce costs and improve outcomes."
      ],
      "metadata": {
        "id": "euXH1J0XDzMB"
      }
    }
  ]
}